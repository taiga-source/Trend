{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import urljoin\n",
    "import base64\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "def sanitize_filename(name):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"\", name)\n",
    "\n",
    "\n",
    "def save_base64_image(data, file_path):\n",
    "    \"\"\"Base64形式の画像をデコードして保存する関数\"\"\"\n",
    "    try:\n",
    "        image_data = base64.b64decode(data)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(image_data)\n",
    "        print(f\"保存成功: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"エラー: {e}\")\n",
    "\n",
    "def download_image(url, file_path):\n",
    "    \"\"\"URLから画像をダウンロードして保存する関数\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            print(f\"保存成功: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"エラー: {e}\")\n",
    "\n",
    "def get_imgs(system, base_url):\n",
    "    # ご自分の環境に合わせて任意のディレクトリパスを指定してください。\n",
    "    base_save_dir = r\"C:\\Users\\NDF05\\OneDrive\\ドキュメント\\VsCodeProject\\Pytyon\\torend\\yuniqro_img\\img\"\n",
    "\n",
    "    base64_string = \"data:image/jpeg;base64,\"\n",
    "    png_base64_string = \"data:image/png;base64,\"\n",
    "\n",
    "\n",
    "    html = driver.page_source.encode(\"utf-8\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "\n",
    "    img_tags = soup.find_all(\"img\")\n",
    "    img_urls = []\n",
    "\n",
    "    for img_tag in img_tags:\n",
    "        url = img_tag.get(\"src\")\n",
    "        if url is None:\n",
    "            url = img_tag.get(\"data-src\")\n",
    "        if url is not None:\n",
    "            if not url.startswith(\"http\"):\n",
    "                url = urljoin(base_url, url)\n",
    "            img_urls.append(url)\n",
    "            print(url)\n",
    "\n",
    "    # 画像を保存\n",
    "    for index, url in enumerate(img_urls):\n",
    "        file_name = \"{}.jpg\".format(index)\n",
    "        image_path = os.path.join(base_save_dir, file_name)\n",
    "\n",
    "        if base64_string in url:\n",
    "            url = url.replace(base64_string, \"\")\n",
    "            save_base64_image(data=url, file_path=image_path)\n",
    "        elif png_base64_string in url:\n",
    "            url = url.replace(png_base64_string, \"\")\n",
    "            save_base64_image(data=url, file_path=image_path)\n",
    "        else:\n",
    "            download_image(url=url, file_path=image_path)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import base64\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin  # 相対URLを絶対URLに変換するためのモジュール\n",
    "from collections import deque\n",
    "\n",
    "def save_base64_image(data, file_path):\n",
    "    data = data + '=' * (-len(data) % 4)\n",
    "    img = base64.b64decode(data.encode())\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(img)\n",
    "\n",
    "def download_image(url, file_path):\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(1024):\n",
    "                f.write(chunk)\n",
    "\n",
    "def summarize_image(img_urls, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    base64_string = \"data:image/jpeg;base64,\"\n",
    "    png_base64_string = \"data:image/png;base64,\"\n",
    "    gif_base64_string = \"data:image/gif;base64,\"\n",
    "\n",
    "    for index, url in enumerate(img_urls):\n",
    "        file_name = \"{}.jpg\".format(index)\n",
    "        image_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "        if url.startswith(base64_string):\n",
    "            url = url.replace(base64_string, \"\")\n",
    "            save_base64_image(data=url, file_path=image_path)\n",
    "        elif url.startswith(png_base64_string):\n",
    "            url = url.replace(png_base64_string, \"\")\n",
    "            save_base64_image(data=url, file_path=image_path)\n",
    "        elif url.startswith(gif_base64_string):\n",
    "            url = url.replace(gif_base64_string, \"\")\n",
    "            save_base64_image(data=url, file_path=image_path)\n",
    "        else:\n",
    "            if not url.startswith(('http://', 'https://')):\n",
    "                # ベースURLを指定して相対URLを絶対URLに変換\n",
    "                url = urljoin('https://www.google.com/', url)\n",
    "            download_image(url=url, file_path=image_path)\n",
    "\n",
    "    print(\"すべての画像をダウンロードしました！\")\n",
    "\n",
    "def image(url='https://www.google.com/search?q=橋本環奈&hl=ja&tbm=isch', save_dir = r\"C:\\Users\\NDF05\\OneDrive\\ドキュメント\\VsCodeProject\\Pytyon\\torend\\yuniqro_img\\test\\test_img\"):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    html = driver.page_source.encode(\"utf-8\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    img_tags = soup.find_all(\"img\")\n",
    "\n",
    "    img_urls = []\n",
    "\n",
    "    for img_tag in img_tags:\n",
    "        img_url = img_tag.get(\"src\")\n",
    "\n",
    "        if img_url is None:\n",
    "            img_url = img_tag.get(\"data-src\")\n",
    "\n",
    "        if img_url is not None:\n",
    "            img_urls.append(img_url)\n",
    "            print(img_url)\n",
    "\n",
    "def check_last_two_elements_equal(lst):\n",
    "    if len(lst) < 2:\n",
    "        return False  # リストが2つ未満の場合は False を返す\n",
    "    \n",
    "    if lst[-1] == lst[-2]:  # リストの最後の2つが同じ場合\n",
    "        print(\"すすんでいないじゃんんんん\")\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#商品単体ページ　→　レビューページ\n",
    "def get_review(url,name):\n",
    "    url1 = url\n",
    "    driver.get(url1)\n",
    "\n",
    "    #画像を取得\n",
    "    scrape_and_save_images()\n",
    "    # aタグすべてを取得\n",
    "    a_tags = driver.find_elements(By.TAG_NAME, 'a')\n",
    "\n",
    "    # 目的のクラス名の一部を探す\n",
    "    target_class_part = 'fr-ec-link-text-ec-renewal fr-ec-link-text--standalone fr-ec-link-text--noticeable-blue fr-ec-link-text fr-ec-cursor-pointer'\n",
    "\n",
    "    # 目的のクラス名のURLをリストで管理\n",
    "    list_url2 = []\n",
    "    for a_tag in a_tags:\n",
    "        seach_class = a_tag.get_attribute('class')\n",
    "        seach_url = a_tag.get_attribute('href')\n",
    "\n",
    "        # 部分一致でクラス名をチェック\n",
    "        if target_class_part in seach_class:\n",
    "            list_url2.append(seach_url)\n",
    "\n",
    "    try:\n",
    "        #レビューページへ行くURL\n",
    "        url2 = list_url2[0]\n",
    "\n",
    "        driver.get(url2)\n",
    "\n",
    "        # 明示的な待機時間の設定\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "\n",
    "        # 初期のスクロール速度設定\n",
    "        scroll_speed = 10\n",
    "        scroll_js_template = \"\"\"\n",
    "        function slowScroll(scroll_speed) {\n",
    "            var scrollHeight = document.body.scrollHeight;\n",
    "            var currentScroll = 0;\n",
    "\n",
    "            function scroll() {\n",
    "                if (currentScroll < scrollHeight) {\n",
    "                    window.scrollBy(0, scroll_speed);\n",
    "                    currentScroll += scroll_speed;\n",
    "                    setTimeout(scroll, 50);\n",
    "                }\n",
    "            }\n",
    "\n",
    "            scroll();\n",
    "        }\n",
    "        slowScroll(%s);\n",
    "        \"\"\"\n",
    "\n",
    "        # 初回のスクロールを実行\n",
    "        scroll_js = scroll_js_template % scroll_speed\n",
    "        driver.execute_script(scroll_js)\n",
    "\n",
    "        # スクロールが完了するまで待機\n",
    "        time.sleep(2)\n",
    "\n",
    "        # 特定の`<span>`タグを取得する（data-testid属性を使用）\n",
    "        span_tags = driver.find_elements(By.TAG_NAME, 'span')\n",
    "        target_class = 'fr-ec-body fr-ec-body--color-primary-dark fr-ec-body--standard fr-ec-text-align-left fr-ec-text-transform-normal'\n",
    "        list_target = [span.text for span in span_tags if span.get_attribute('class') == target_class]\n",
    "\n",
    "        if not list_target:\n",
    "            print(\"指定されたクラスを持つ<span>タグが見つかりませんでした\")\n",
    "            driver.quit()\n",
    "            return\n",
    "\n",
    "        span_text = list_target[0]\n",
    "\n",
    "        # 数字部分だけを抽出\n",
    "        review_count = int(re.findall(r'\\d+', span_text)[0])\n",
    "        print('全体' + str(review_count))\n",
    "\n",
    "        crent_count = 0\n",
    "        max_length = 10  # リストの最大長\n",
    "        last_known_scroll_position_list = 0\n",
    "        last_known_scroll_position_list = deque(maxlen=max_length)\n",
    "        crent_list = deque(maxlen=max_length)\n",
    "        last_known_scroll_position = 0  # ここで初期化\n",
    "        same_position_count = 0  # 同じ位置が続く回数をカウントする変数\n",
    "        scroll_speed = 0\n",
    "        max_valueL = 3000\n",
    "\n",
    "        while crent_count < review_count:\n",
    "            crent_list.append(crent_count)\n",
    "            print(crent_list)\n",
    "\n",
    "            max_value = max(crent_list)\n",
    "            count = 0\n",
    "\n",
    "            for num in crent_list:\n",
    "                if num == max_value:\n",
    "                    count += 1\n",
    "                    if count == 5:\n",
    "                        break\n",
    "            \n",
    "            # crent_countを増やして、無限ループを防ぐ\n",
    "            crent_count += 1\n",
    "            print(crent_count)\n",
    "\n",
    "            if max_valueL == max(crent_list):\n",
    "                print(\"上限取得を突破したのでやめる！！\")\n",
    "                break\n",
    "            # 最大値が5つ重複した場合は、whileループを抜ける\n",
    "            if count == 5:\n",
    "                print(\"最大値が5つ重複するとやめる！！\")\n",
    "                break\n",
    "\n",
    "            # スクロールを実行\n",
    "            driver.execute_script(scroll_js)\n",
    "\n",
    "            # スクロールが完了するまで待機\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # 現在のスクロール位置を取得\n",
    "            current_scroll_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "            last_known_scroll_position_list.append(current_scroll_position)\n",
    "            print(last_known_scroll_position_list)\n",
    "            print(f\"現在のスクロール位置: {current_scroll_position}\")\n",
    "\n",
    "            if current_scroll_position < last_known_scroll_position:\n",
    "                print(\"初期化している！！！！！\")\n",
    "                driver.execute_script(f\"window.scrollTo(0, {last_known_scroll_position});\")\n",
    "                time.sleep(2)  # スクロール後に待機時間を設ける\n",
    "            else:\n",
    "                last_known_scroll_position = current_scroll_position\n",
    "                #todo#last_known_scroll_position_list.append(last_known_scroll_position)\n",
    "                ######last_known_scroll_position = max(last_known_scroll_position_list)\n",
    "\n",
    "            # 前回とのスクロール位置の差を計算\n",
    "            if len(last_known_scroll_position_list) > 1:\n",
    "                scroll_difference = last_known_scroll_position_list[-1] - last_known_scroll_position_list[-2]\n",
    "                print(f\"スクロール位置の差: {scroll_difference}\")\n",
    "                \n",
    "                # 差が小さい場合にスクロール速度を減少させる\n",
    "                if abs(scroll_difference) < 500:  # 500は適切な値に調整可能\n",
    "                    scroll_speed = max(1, scroll_speed - 2)  # 最小速度は1\n",
    "                    scroll_js = scroll_js_template % scroll_speed\n",
    "                    print(f\"スクロール速度を低下させました: {scroll_speed}\")\n",
    "\n",
    "            if len(last_known_scroll_position_list) > 1 and last_known_scroll_position_list[-1] == last_known_scroll_position_list[-2]:\n",
    "                same_position_count += 1  # 同じ位置が続いたらカウントを増やす\n",
    "            else:\n",
    "                same_position_count = 0  # 異なる位置に移動したらカウントをリセット\n",
    "\n",
    "            if same_position_count >= 5:  # 5回同じ位置が続いたら処理を停止\n",
    "                print(\"やめる\")\n",
    "                crent_count = review_count\n",
    "                break\n",
    "\n",
    "            # 同じ位置に留まっているか確認\n",
    "            if check_last_two_elements_equal(last_known_scroll_position_list):\n",
    "                print(\"同じ位置で停止しているため、スクロール位置を調整します\")\n",
    "                unique_max_value = max(set(last_known_scroll_position_list) - {max(last_known_scroll_position_list)})\n",
    "                last_known_scroll_position = unique_max_value  # 前の位置に戻る\n",
    "                driver.execute_script(f\"window.scrollTo(0, {last_known_scroll_position});\")\n",
    "                time.sleep(2)\n",
    "            \n",
    "            # レビューの数を数える\n",
    "            li_tags = driver.find_elements(By.CSS_SELECTOR, \"li\")\n",
    "            target_class = 'fr-ec-collection-list__item fr-ec-collection-list__item-spacing-16 fr-ec-collection-list__item--keylines-bottom'\n",
    "            crent_count = sum(1 for li in li_tags if li.get_attribute('class') == target_class)\n",
    "            print('現在' + str(crent_count))\n",
    "            print(last_known_scroll_position_list)\n",
    "            print(same_position_count)\n",
    "\n",
    "        # スクロールが完了するまで待機\n",
    "        time.sleep(2)\n",
    "\n",
    "        # ページ全体のレビュー要素を取得\n",
    "        review_elements = driver.find_elements(By.CSS_SELECTOR, 'li.fr-ec-collection-list__item')\n",
    "\n",
    "        # レビューのテキストと星の数を取得して表示\n",
    "        reviews = [(review_element.text, len(review_element.find_elements(By.CSS_SELECTOR, '.fr-ec-star--full')))\n",
    "                for review_element in review_elements]\n",
    "\n",
    "        # レビューをコンソールに出力\n",
    "        for review, stars in reviews:\n",
    "            print(f'Review: {review}\\nStars: {stars}\\n')\n",
    "\n",
    "        # ファイルに保存\n",
    "        name = sanitize_filename(name)\n",
    "        file_path = fr'C:\\Users\\NDF05\\OneDrive\\ドキュメント\\VsCodeProject\\Pytyon\\torend\\merchandises\\{name}.txt'\n",
    "\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            for review, stars in reviews:\n",
    "                f.write(f'Review: {review}\\nStars: {stars}\\n\\n')\n",
    "\n",
    "        time.sleep(2)\n",
    "        driver.back()\n",
    "\n",
    "    except IndexError:\n",
    "        print(\"からあああ！\")\n",
    "        driver.back()\n",
    "\n",
    "#商品画像を取得\n",
    "def scrape_and_save_images():\n",
    "\n",
    "    # imgタグから画像のURLを取得\n",
    "    imgs_tags = driver.find_elements(By.TAG_NAME, 'img')\n",
    "    target_list = []\n",
    "    target_alts = ['image-{}'.format(i) for i in range(5)]\n",
    "\n",
    "    for img_tag in imgs_tags:\n",
    "        seach_alt = img_tag.get_attribute('alt')  # 修正箇所\n",
    "        if seach_alt in target_alts:\n",
    "            seach_url = img_tag.get_attribute('src')  # 修正箇所\n",
    "            target_list.append(seach_url)\n",
    "\n",
    "    print(target_list)\n",
    "\n",
    "    # 画像を保存するディレクトリ\n",
    "    save_dir = r\"C:\\Users\\NDF05\\OneDrive\\ドキュメント\\VsCodeProject\\Pytyon\\torend\\yuniqro_img\\test\\test_img\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 商品名を取得\n",
    "    h1_tags = driver.find_elements(By.TAG_NAME, 'h1')\n",
    "    target_class_part = 'fr-ec-display fr-ec-display--color-primary-dark fr-ec-display--display5 fr-ec-text-align-left fr-ec-text-transform-normal'\n",
    "    product_name = None\n",
    "\n",
    "    for h1_tag in h1_tags:\n",
    "        seach_class = h1_tag.get_attribute('class')\n",
    "        if target_class_part in seach_class:\n",
    "            product_name = h1_tag.text\n",
    "            break\n",
    "\n",
    "    if not product_name:\n",
    "        print(\"商品名が見つかりませんでした。\")\n",
    "        driver.quit()\n",
    "    \n",
    "    product_name = sanitize_filename(product_name)\n",
    "    print(\"tio\"+ product_name)\n",
    "\n",
    "    # 複数の画像をダウンロード\n",
    "    for index, url in enumerate(target_list):\n",
    "        file_path = os.path.join(save_dir, f\"{product_name}_{index}.jpg\")\n",
    "        download_image(url=url, file_path=file_path)\n",
    "\n",
    "\n",
    "#商品カテゴリのページ→各商品のページ\n",
    "def merchandises(url_x):\n",
    "    #print(\"tio 商品をスクロール中！！！\")\n",
    "\n",
    "    driver.get(url_x)\n",
    "    # 明示的な待機時間の設定（30秒に延長）\n",
    "    wait = WebDriverWait(driver, 120)\n",
    "\n",
    "    # 緩やかにスクロールするJavaScript関数を定義\n",
    "    scroll_js = \"\"\"\n",
    "    function slowScroll() {\n",
    "        var scrollHeight = document.body.scrollHeight;\n",
    "        var currentScroll = 0;\n",
    "        var scrollIncrement = 10;  // 小さな増分でスクロールする（ピクセル）\n",
    "        \n",
    "        function scroll() {\n",
    "            if (currentScroll < 100000) {\n",
    "                window.scrollBy(0, scrollIncrement);\n",
    "                currentScroll += scrollIncrement;\n",
    "                console.log(currentScroll);  // 現在のスクロール位置をコンソールに出力\n",
    "                setTimeout(scroll, 50);  // 50ミリ秒ごとにスクロールする\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        scroll();  // スクロール関数を実行\n",
    "    }\n",
    "    slowScroll();\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # スクロールを実行\n",
    "    driver.execute_script(scroll_js)\n",
    "\n",
    "    # スクロールが完了するまで待機\n",
    "    time.sleep(120)  # 必要に応じて適切な待機時間を設定\n",
    "\n",
    "    #画像の取得\n",
    "    # imgs_tags = driver.find_elements(By.TAG_NAME, 'img')\n",
    "    # print(\"tio\")\n",
    "    \n",
    "\n",
    "    # target_list = []\n",
    "    # for i in range(len(imgs_tags)):\n",
    "    #     seach_url  = imgs_tags[i].get_attribute('src')\n",
    "    #     target_list.append(seach_url)\n",
    "\n",
    "    # print(target_list)\n",
    "\n",
    "    # save_dir = r'C:\\Users\\NDF05\\OneDrive\\ドキュメント\\VsCodeProject\\Pytyon\\torend\\yuniqro_img\\img'\n",
    "    # summarize_image(target_list, save_dir)\n",
    "\n",
    "    # for i in range(len(target_list)):\n",
    "        # get_imgs(target_list[i])\n",
    "        # time.sleep(2)\n",
    "\n",
    "\n",
    "    # すべての「もっと見る」ボタンをクリックする\n",
    "    while True:\n",
    "        try:\n",
    "            more_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'button[aria-label=\"Load More\"]')))\n",
    "            more_button.click()\n",
    "            time.sleep(2)  # ボタンがクリックされるのを待つ\n",
    "        except TimeoutException as e:\n",
    "            print(f\"No more buttons to click: {e}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "\n",
    "    # ページが完全に読み込まれるまで待機（CSSセレクターを使用）\n",
    "    wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a.fr-ec-product-tile')))\n",
    "\n",
    "    # aタグすべてを取得\n",
    "    a_tags = driver.find_elements(By.TAG_NAME, 'a')\n",
    "\n",
    "    # 目的のクラス名の一部を探す\n",
    "    target_class_part = 'fr-ec-product-tile'\n",
    "\n",
    "    # 目的のクラス名のURLをリストで管理\n",
    "    list_url = []\n",
    "    for a_tag in a_tags:\n",
    "        seach_class = a_tag.get_attribute('class')\n",
    "        seach_url = a_tag.get_attribute('href')\n",
    "\n",
    "        # 部分一致でクラス名をチェック\n",
    "        if target_class_part in seach_class:\n",
    "            list_url.append(seach_url)\n",
    "\n",
    "    #商品名の取得\n",
    "    h3_tags = driver.find_elements(By.TAG_NAME, 'h3')\n",
    "\n",
    "    # 目的のクラス名の一部を探す\n",
    "    target_class_part = 'fr-ec-title fr-ec-title--color-primary-dark fr-ec-title--product-tile-horizontal fr-ec-text-align-left fr-ec-text-transform-normal fr-ec-product-tile__end-product-name'\n",
    "\n",
    "    # h3\n",
    "    list_name = []\n",
    "    for i in range(len(h3_tags)):\n",
    "        try:\n",
    "            seach_class = h3_tags[i].get_attribute('class')\n",
    "\n",
    "            # 部分一致でクラス名をチェック\n",
    "            if target_class_part in seach_class:\n",
    "                print(h3_tags[i].text)\n",
    "                list_name.append(h3_tags[i].text)\n",
    "\n",
    "        except StaleElementReferenceException:\n",
    "            continue\n",
    "    merchandise_num = 2\n",
    "\n",
    "    for i in range(len(list_name)):\n",
    "        url1 = list_url[i]\n",
    "        print(\"商品単体ページにきたあああ！！！！！\")\n",
    "        # scrape_and_save_images()\n",
    "\n",
    "\n",
    "        get_review(url1, list_name[i])\n",
    "\n",
    "#すべての商品のカテゴリのURLを取得する\n",
    "def get_all_URL():\n",
    "    # WebDriverの作成\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # URLを開く\n",
    "    driver.get(\"https://www.uniqlo.com/jp/ja/\")\n",
    "\n",
    "    driver.find_element(By.CSS_SELECTOR, \".fr-ec-bottom-navigation__icon-wrapper-center > .fr-ec-icon\").click()\n",
    "\n",
    "    # 明示的な待機時間の設定（30秒に延長）\n",
    "    wait = WebDriverWait(driver, 30)\n",
    "\n",
    "    # divタグすべてを取得\n",
    "    div_tags = driver.find_elements(By.TAG_NAME, 'div')\n",
    "\n",
    "    list_target = []\n",
    "    target_class = 'fr-ec-class-item'\n",
    "    for div_tag in div_tags:\n",
    "        seach_class = div_tag.get_attribute('class')\n",
    "        if seach_class and target_class in seach_class:\n",
    "            list_target.append(div_tag)\n",
    "\n",
    "    # リストの2番目の要素をクリック\n",
    "    if len(list_target) > 1:\n",
    "        list_target[1].click()\n",
    "    else:\n",
    "        print(\"対象の要素が見つかりませんでした\")\n",
    "\n",
    "\n",
    "    # 「.fr-ec-bottom-navigation__icon-wrapper-center > .fr-ec-icon」アイコンをクリック\n",
    "    WebDriverWait(driver, 30).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \".fr-ec-bottom-navigation__icon-wrapper-center > .fr-ec-icon\"))\n",
    "    ).click()\n",
    "\n",
    "    # 明示的な待機時間の設定（30秒に延長）\n",
    "    wait = WebDriverWait(driver, 30)\n",
    "\n",
    "    # 特定のクラス名を持つ親要素を取得\n",
    "    parent_element = driver.find_element(By.CLASS_NAME, 'fr-ec-class-list__category-list-wrapper')\n",
    "\n",
    "    # 子要素を再帰的に取得する関数\n",
    "    def get_all_child_urls(element):\n",
    "        children = element.find_elements(By.TAG_NAME, 'a')\n",
    "        urls = []\n",
    "        for child in children:\n",
    "            href = child.get_attribute('href')\n",
    "            if href:\n",
    "                urls.append(href)\n",
    "\n",
    "        return urls\n",
    "\n",
    "    # 子要素のURLを取得して表示\n",
    "    list_urls = get_all_child_urls(parent_element)\n",
    "\n",
    "    a_tags = driver.find_elements(By.TAG_NAME, 'a')\n",
    "\n",
    "    url_list = []\n",
    "    for i in range(len(a_tags)):\n",
    "        hifes = a_tags[i].get_attribute('href')\n",
    "        url_list.append(hifes)\n",
    "\n",
    "    lest_target_u = ['tops',\n",
    "                    'bottoms',\n",
    "                    'dresses-and-skirts',\n",
    "                    'outerwear',\n",
    "                    'innerwear',\n",
    "                    'loungewear-and-homeware',\n",
    "                    'accessories',\n",
    "                    'maternity',\n",
    "                    'airism',\n",
    "                    'formal-school',]\n",
    "\n",
    "    list_urls2 = []\n",
    "    for seach_url in url_list:\n",
    "        if seach_url is None:\n",
    "            print(\"Skipping None value\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        # URLを\"/\"で分割して目的の要素を抽出\n",
    "        elements = seach_url.split(\"/\")\n",
    "\n",
    "        # Check if there are enough elements in the list\n",
    "        if len(elements) > 7:\n",
    "            desired_elements = elements[5:8]  # \"women\", \"tops\", \"t-shirts\"\n",
    "\n",
    "            if desired_elements[1] in lest_target_u:\n",
    "                list_urls2.append(seach_url)\n",
    "        else:\n",
    "            print(\"URL does not have enough elements:\", seach_url)\n",
    "\n",
    "\n",
    "    list_urls2 = []\n",
    "    for seach_url in url_list:\n",
    "        if seach_url is None:\n",
    "            print(\"Skipping None value\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        # URLを\"/\"で分割して目的の要素を抽出\n",
    "        elements = seach_url.split(\"/\")\n",
    "\n",
    "        # Check if there are enough elements in the list\n",
    "        if len(elements) > 7:\n",
    "            desired_elements = elements[5:8]  # \"women\", \"tops\", \"t-shirts\"\n",
    "            print(desired_elements[1])\n",
    "\n",
    "            if desired_elements[1] in lest_target_u:\n",
    "                list_urls2.append(seach_url)\n",
    "        else:\n",
    "            print(\"URL does not have enough elements:\", seach_url)\n",
    "\n",
    "    return list_urls2\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import re\n",
    "\n",
    "\n",
    "# ヘッドレスモードの設定\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('--window-size=1920x1080')\n",
    "options.add_argument('--no-sandbox')\n",
    "\n",
    "# WebDriverの作成\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# URLを開く\n",
    "driver.get(\"https://www.uniqlo.com/jp/ja/\")\n",
    "\n",
    "#すべての商品のカテゴリのURLを取得する\n",
    "list_URL = get_all_URL()\n",
    "\n",
    "for i in range(len(list_URL)):\n",
    "    merchandises(list_URL[i])\n",
    "    driver.back()\n",
    "    driver.back()\n",
    "    time.sleep(120) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
