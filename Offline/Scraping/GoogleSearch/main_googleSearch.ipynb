{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from urllib.parse import urljoin\n",
    "import base64\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "def save_base64_image(data, file_path):\n",
    "    \"\"\"Base64形式の画像をデコードして保存する関数\"\"\"\n",
    "    try:\n",
    "        image_data = base64.b64decode(data)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(image_data)\n",
    "        print(f\"保存成功: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"エラー: {e}\")\n",
    "\n",
    "def download_image(url, file_path):\n",
    "    \"\"\"URLから画像をダウンロードして保存する関数\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            print(f\"保存成功: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"エラー: {e}\")\n",
    "\n",
    "def get_imgs(system):\n",
    "    # ご自分の環境に合わせて任意のディレクトリパスを指定してください。\n",
    "    base_save_dir = r\"C:\\Users\\NDF05\\OneDrive\\ドキュメント\\VsCodeProject\\Pytyon\\torend\\Select_imegMnager\\Slect_imgs\"\n",
    "    save_dir = os.path.join(base_save_dir, system)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    base64_string = \"data:image/jpeg;base64,\"\n",
    "    png_base64_string = \"data:image/png;base64,\"\n",
    "\n",
    "\n",
    "    # 検索クエリとURLの生成\n",
    "    query = system +\" 服 imagesize:1080x1080\"\n",
    "    url = \"https://www.google.com/search?q={}&hl=ja&tbm=isch\".format(query)\n",
    "\n",
    "    # Chromeドライバの設定\n",
    "    service = Service(executable_path='path_to_chromedriver')  # chromedriverのパスを指定してください\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    # 緩やかにスクロールするJavaScript関数を定義\n",
    "    scroll_js = \"\"\"\n",
    "    function slowScroll() {\n",
    "        var scrollHeight = document.body.scrollHeight;\n",
    "        var currentScroll = 0;\n",
    "        var scrollIncrement = 10;  // 小さな増分でスクロールする（ピクセル）\n",
    "        \n",
    "        function scroll() {\n",
    "            if (currentScroll < 100000) {\n",
    "                window.scrollBy(0, scrollIncrement);\n",
    "                currentScroll += scrollIncrement;\n",
    "                console.log(currentScroll);  // 現在のスクロール位置をコンソールに出力\n",
    "                setTimeout(scroll, 50);  // 50ミリ秒ごとにスクロールする\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        scroll();  // スクロール関数を実行\n",
    "    }\n",
    "    slowScroll();\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # スクロールを実行\n",
    "    driver.execute_script(scroll_js)\n",
    "    time.sleep(60*2)  # ページが読み込まれるまで待つ\n",
    "\n",
    "    html = driver.page_source.encode(\"utf-8\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # ベースURLの取得\n",
    "    base_url = \"https://www.google.com\"\n",
    "\n",
    "    img_tags = soup.find_all(\"img\")\n",
    "    img_urls = []\n",
    "\n",
    "    for img_tag in img_tags:\n",
    "        url = img_tag.get(\"src\")\n",
    "        if url is None:\n",
    "            url = img_tag.get(\"data-src\")\n",
    "        if url is not None:\n",
    "            if not url.startswith(\"http\"):\n",
    "                url = urljoin(base_url, url)\n",
    "            img_urls.append(url)\n",
    "            print(url)\n",
    "\n",
    "    # 画像を保存\n",
    "    for index, url in enumerate(img_urls):\n",
    "        file_name = \"{}.jpg\".format(index)\n",
    "        image_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "        if base64_string in url:\n",
    "            url = url.replace(base64_string, \"\")\n",
    "            save_base64_image(data=url, file_path=image_path)\n",
    "        elif png_base64_string in url:\n",
    "            url = url.replace(png_base64_string, \"\")\n",
    "            save_base64_image(data=url, file_path=image_path)\n",
    "        else:\n",
    "            download_image(url=url, file_path=image_path)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    \n",
    "casual_styles = [\n",
    "    \"ベーシック\",\n",
    "    \"アメリカンカジュアル（アメカジ）\",\n",
    "    \"モードカジュアル\",\n",
    "    \"スポーツカジュアル\",\n",
    "    \"ストリートカジュアル\",\n",
    "    \"アウトドア\"\n",
    "]\n",
    "\n",
    "elegant_styles = [\n",
    "    \"エレガンス\",\n",
    "    \"ガーリー\",\n",
    "    \"オフィスカジュアル\",\n",
    "    \"キャリア\",\n",
    "    \"マニッシュ\",\n",
    "    \"デザイナーズ\"\n",
    "]\n",
    "\n",
    "other_styles = [\n",
    "    \"トラッド\",\n",
    "    \"ノームコア\",\n",
    "    \"ナチュラル\",\n",
    "    \"韓国（オルチャン）\",\n",
    "    \"量産型\",\n",
    "    \"地雷系\"\n",
    "]\n",
    "\n",
    "sum_list = casual_styles + elegant_styles + other_styles\n",
    "\n",
    "for i in range(len(sum_list)):\n",
    "    get_imgs(sum_list[i])\n",
    "    time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
